{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "incredible-tennessee",
   "metadata": {},
   "source": [
    "# Lab 3: Evaluation of the model\n",
    "\n",
    "***Author: Martín Cepeda***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "addressed-service",
   "metadata": {},
   "source": [
    "## Part 1: Fixing Teaching Forcing\n",
    "\n",
    "### Beam Search\n",
    "The original beam-search strategy finds a translation that approximately maximizes the conditional probability given by a specific model. It builds the translation from left-to-right and keeps a fixed number (beam) of translation candidates with the highest log-probability at each time step. For each end-of-sequence symbol that is selected among the highest scoring candidates the beam is reduced by one and the translation is stored into a final candidate list. When the beam is zero, it stops the search and picks the translation with the highest log-probability (normalized by the number of target words) out of the final candidate list[.](https://arxiv.org/pdf/1702.01806.pdf)\n",
    "\n",
    "**Trade-off:** By setting the beam size large enough, we ensure that the best translation performance can be reached with the drawback that many candidates whose scores are far away from the best are also explored.\n",
    "\n",
    "\n",
    "### Curriculum Learning (Scheduled Sampling)\n",
    "\n",
    "At an abstract level, a curriculum can be seen as a sequence of training criteria. Each training criterion in the sequence is associated with a different set of weights on the training examples, or more generally, on a reweighting of the training distribution. Initially, the weights favor “easier” examples, or examples illustrating the simplest concepts, that can be learned most easily. The next training criterion involves a slight change in the weighting of examples that increases the\n",
    "probability of sampling slightly more difficult examples. At the end of the sequence, the reweighting of the examples is uniform and we train on the target training set or the target training distribution[.](http://machinelearning.org/archive/icml2009/papers/119.pdf)\n",
    "\n",
    "\n",
    "### Parallel Scheduled Sampling\n",
    "Scheduled Sampling (Bengio et al., 2015) is a training technique designed to bridge the gap between teacher-forcing and sample decoding. In its simplest form, Sequential Scheduled Sampling generates tokens $\\tilde y_{1:t}$ and conditions on these target prefixes during training. Sequential Scheduled Sampling uses the same objective function as teacher-forcing except the conditioning tokens $\\tilde y_{1:t}$ are a random mixture of gold tokens $ y_{1:t}$ and sampled tokens $\\hat y_{1:t}$ instead of gold tokens $ y_{1:t}$.\n",
    "\n",
    "Whereas Sequential Scheduled Sampling selects conditioning tokens one after another, Parallel Scheduled Sampling consists on generating conditioning tokens for all timesteps in parallel over the course of one or more passes. While this technique requires strictly more operations than Sequential Scheduled Sampling, it is better suited to hardware accelerators such as GPUs and TPUs. The procedure consists of multiple passes, each pass consisting of parallel sampling and mixing steps[.](https://arxiv.org/pdf/1906.04331.pdf)\n",
    "\n",
    "\n",
    "### Professor Forcing\n",
    "\n",
    "The basic idea of Professor Forcing is simple: while we do want the generative RNN to match thetraining data, we also want the behavior of the network (both in its outputs and in the dynamics of its hidden states) to be indistinguishable whether the network is trained with its inputs clamped to a training sequence (teacher forcing mode) or whether its inputs are self-generated (free-running generative mode). Because we can only compare the distribution of these sequences, it makes sense\n",
    "to take advantage of the generative adversarial networks (GANs) framework to achieve that second objective of matching the two distributions over sequences (the one observed in teacher forcing mode vs the one observed in free-running mode)[.](https://arxiv.org/pdf/1610.09038.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dated-presentation",
   "metadata": {},
   "source": [
    "## Part 2: \"Improved\" model\n",
    "\n",
    "This part is mostly qualitative. Whereas I re-implemented the model presented in Lab 2 including attention, the results are more than catastrophic. This is due to several reasons:\n",
    "\n",
    "a) no real fixing of exposure bias\n",
    "\n",
    "b) scarce number of training epochs due to limited computing time/power\n",
    "\n",
    "c) rather small dataset considering that we're trying to accomplish a variant of machine translation\n",
    "\n",
    "Nevertheless, the sources can also be traced back to feature selection: sampling rate for fundamental frequency, robustness of OpenFace to extract action features or the sufficiency of F0 to explain the different AUs. In [this paper](https://arxiv.org/ftp/arxiv/papers/1810/1810.12541.pdf) they explore a conceptually similar approach but based on text, which has a much richer domain and semantics than the discretization of F0, whereas in [this paper](https://link.springer.com/article/10.1007/s11263-019-01251-8) they leverage visual information (not AUs) to make a GAN speech-to-gesture synthesizer. Again, we observe a bigger feature domain that could ultimately have AUs as a side product once the gesture image is produced. Similarly, in [this paper](https://dl.acm.org/doi/10.1145/3072959.3073658) they explore the use of non-prosodic audio features to learn the 3D mesh extracted from video. Also, they redefine the loss (as they're dealing with points in R^3).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "deluxe-video",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.config.run_functions_eagerly(True)\n",
    "from tensorflow import keras\n",
    "\n",
    "from pprint import pprint\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "experienced-values",
   "metadata": {},
   "source": [
    "### Path setup\n",
    "\n",
    "This section sets the relative paths to:\n",
    "\n",
    "- Search for clean features\n",
    "- Set paths for saving model checkpoints and training history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "democratic-corrections",
   "metadata": {},
   "outputs": [],
   "source": [
    "here = os.getcwd()\n",
    "\n",
    "base_dir = os.path.join(here, \"data\")\n",
    "batch1_dir = os.path.join(base_dir, \"batch 1\")\n",
    "batch2_dir = os.path.join(base_dir, \"batch 2\")\n",
    "figures_dir = os.path.join(here, \"figures\", \"lab3\")\n",
    "\n",
    "debug_dir = os.path.join(here, \"debug\")\n",
    "model_dir = os.path.join(debug_dir, \"model\")\n",
    "history_dir = os.path.join(debug_dir, \"history\")\n",
    "\n",
    "os.makedirs(debug_dir, exist_ok=True)\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "os.makedirs(history_dir, exist_ok=True)\n",
    "os.makedirs(figures_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "indonesian-connectivity",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available videos:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['k4vzhweOefs',\n",
       " 'lr-mXnUoUXM',\n",
       " 'O6jrLgvCUNs',\n",
       " 'ovKqmRyOGcg',\n",
       " 'psN1DORYYV0',\n",
       " 'tZYkjaKNr_o',\n",
       " 'XE_FPEFpHt4',\n",
       " 'yCm9Ng0bbEQ',\n",
       " 'zawpbVpu5nY',\n",
       " 'ZdDjexbxVzM',\n",
       " '2tBuvxXxlS4',\n",
       " '3boKz0Exros',\n",
       " '5RAJvzV9j-o',\n",
       " '6LmPq7D-ds0',\n",
       " '6OaIdwUdSxE',\n",
       " '6We_1bXRBOk',\n",
       " 'd6NKdnZvdoo',\n",
       " 'E3cK8IL0JCE',\n",
       " 'eD9F5HdyKqU',\n",
       " 'g3vSYbT1Aco',\n",
       " 'MvXZzKZ3JYQ',\n",
       " 'NP8xt8o4_5Q',\n",
       " 'urntcMUJR9M',\n",
       " 'ux1GxExRUUY',\n",
       " 'wD3-6JIUF7M',\n",
       " 'y9ALB39wRKo']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch1_ids = [os.path.split(file)[1][:-4] for file in glob(os.path.join(batch1_dir, \"*.csv\"))]\n",
    "batch2_ids = [os.path.split(file)[1][:-4] for file in glob(os.path.join(batch2_dir, \"*.csv\"))]\n",
    "\n",
    "video_ids = batch1_ids + batch2_ids\n",
    "\n",
    "print(\"Available videos:\")\n",
    "video_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "velvet-russia",
   "metadata": {},
   "source": [
    "### Loading clean data\n",
    "\n",
    "In order to make our dataset, we must make windows of size 100 **per each scene** in ell videos. In order to do this, we can merge all video CSVs one unique dataframe and identify all scenes, keeping track of video IDs and frame numbers to recover the correspondant $F0$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dedicated-sucking",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 26/26 [00:01<00:00, 14.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values:\n",
      "frame        0\n",
      "timestamp    0\n",
      "AU01_r       0\n",
      "AU02_r       0\n",
      "AU04_r       0\n",
      "AU05_r       0\n",
      "AU06_r       0\n",
      "AU07_r       0\n",
      "f0           0\n",
      "video_id     0\n",
      "dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Merge all dataframes\n",
    "df_AU = pd.DataFrame()\n",
    "\n",
    "for i, video_id in enumerate(tqdm(video_ids)):\n",
    "    # Infer directory\n",
    "    batch_dir_i = batch1_dir if i < len(batch1_ids) else batch2_dir\n",
    "    # Read video dataframe\n",
    "    df_i = pd.read_csv(os.path.join(batch_dir_i, f\"{video_id}.csv\"))\n",
    "    # Read audio dataframe\n",
    "    df_f0_i = pd.read_csv(os.path.join(batch_dir_i, f\"{video_id}.f0.txt\"),\n",
    "                         sep=\"\\t\",\n",
    "                         names=[\"timestamp\", \"f0\"])\n",
    "    \n",
    "    # Merge on timestamps of AU dataframe\n",
    "    df_i = pd.merge_asof(df_i, df_f0_i, on=\"timestamp\")\n",
    "    # Add video id for future tracking\n",
    "    df_i['video_id'] = video_id\n",
    "    # Append to total dataframe\n",
    "    df_AU = df_AU.append(df_i)\n",
    "\n",
    "df_AU.reset_index(drop=True, inplace=True)\n",
    "# Drop success column as data is clean\n",
    "df_AU.drop(\"success\", axis=1, inplace=True)\n",
    "\n",
    "print(\"Missing values:\")\n",
    "print(df_AU.isna().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deadly-gates",
   "metadata": {},
   "source": [
    "Scene extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "sustained-browser",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_scene_idxs(df):\n",
    "    \"\"\"Get beginning and end indexes for every scene in dataframe.\n",
    "    A scene is defined by a series of consecutive frames\n",
    "    \"\"\"\n",
    "    # Get idxs where there are no consecutive frames\n",
    "    end_of_scenes = list(np.where(df['frame'].diff().bfill() != 1)[0])\n",
    "    end_of_scenes.append(len(df))\n",
    "    \n",
    "    # Fill tuples (frame_beginning, frame_end) for each scene\n",
    "    scene_idxs = []\n",
    "    for i, idx in enumerate(end_of_scenes):\n",
    "        if i == 0:\n",
    "            beg_i = 0  \n",
    "        else:\n",
    "            beg_i = end_of_scenes[i-1]\n",
    "        \n",
    "        end_i = idx\n",
    "        \n",
    "        scene_idxs.append((beg_i, end_i))\n",
    "    # Return list of scene indexes\n",
    "    return scene_idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "solar-florence",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Description of the length of scenes:\n",
      "count     590.000000\n",
      "mean      522.869492\n",
      "std       538.166413\n",
      "min        51.000000\n",
      "25%       218.000000\n",
      "50%       379.500000\n",
      "75%       628.500000\n",
      "max      6971.000000\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "scene_idxs = get_scene_idxs(df_AU)\n",
    "\n",
    "# Show and describe scene lengths\n",
    "scene_lengths = pd.Series([id_f - id_i for (id_i, id_f) in scene_idxs])\n",
    "\n",
    "plt.hist(scene_lengths, bins=20)\n",
    "plt.title(\"Histogram of scene duration\")\n",
    "plt.xlabel(\"Length\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.savefig(os.path.join(figures_dir, \"scene_hist.png\"), bbox_inches='tight', facecolor=\"white\",transparent=False)\n",
    "plt.show()\n",
    "\n",
    "print(\"Description of the length of scenes:\")\n",
    "print(scene_lengths.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "double-sleep",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check: all scenes span always an unique video\n",
    "for (id_i, id_f) in scene_idxs:\n",
    "    if df_AU[id_i:id_f]['video_id'].nunique() != 1:\n",
    "        print(f\"Problem in scene between {id_i} and {id_f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "engaging-uncle",
   "metadata": {},
   "source": [
    "### Make sequence dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "double-liberia",
   "metadata": {},
   "source": [
    "We define a quantization function for AU intensity following the formula:\n",
    "$$Q_\\Delta(x) = \\Delta\\cdot\\left \\lfloor{\\frac{x}{\\Delta}+\\frac{1}{2}}\\right \\rfloor $$\n",
    "\n",
    "Which rounds $x$ to the nearest multiple of $\\Delta$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fixed-correspondence",
   "metadata": {},
   "outputs": [],
   "source": [
    "def quantize(x, step=0.1):\n",
    "    \"\"\"Quantize a scalar or a numpy array with a fixed step size\n",
    "    \"\"\"\n",
    "    assert step > 0.0\n",
    "    return np.round(step*np.floor(x/step +0.5), 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "documentary-teach",
   "metadata": {},
   "source": [
    "Quantization parameters for AU and F0, sequence parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "neural-outline",
   "metadata": {},
   "outputs": [],
   "source": [
    "STEP_SIZE_AU = 0.2\n",
    "STEP_SIZE_F0 = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ruled-usage",
   "metadata": {},
   "source": [
    "Perform quantization on AU and F0, also clipping the latter between 50 and 550"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "knowing-ticket",
   "metadata": {},
   "outputs": [],
   "source": [
    "AU_columns = [col for col in df_AU.columns if \"AU\" in col]\n",
    "\n",
    "df_AU[AU_columns] = df_AU[AU_columns].applymap(lambda x: quantize(x, STEP_SIZE_AU))\n",
    "df_AU[\"f0\"] = df_AU[\"f0\"].apply(lambda x: quantize(np.clip(x, 50, 550), STEP_SIZE_F0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "norman-registration",
   "metadata": {},
   "source": [
    "Make vocabularies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "directed-amino",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary sizes\n",
      "AU: 26\n",
      "F0: 3605\n"
     ]
    }
   ],
   "source": [
    "all_AU = set()\n",
    "for col in AU_columns:\n",
    "    all_AU = all_AU.union(df_AU[col].values)\n",
    "    \n",
    "ID_to_AU = {0: \"PAD\",\n",
    "            1: \"SOS\",\n",
    "            2: \"EOS\"}\n",
    "for i, value in enumerate(sorted(list(all_AU))):\n",
    "    ID_to_AU[i+3] = str(value)\n",
    "    \n",
    "all_f0 = set(df_AU[\"f0\"].values)\n",
    "ID_to_F0 = {0: \"PAD\",\n",
    "            1: \"SOS\",\n",
    "            2: \"EOS\"}\n",
    "for i, value in enumerate(sorted(list(all_f0))):\n",
    "    ID_to_F0[i+3] = str(value)\n",
    "\n",
    "# Make reverse_search dictionaries\n",
    "AU_to_ID = {v:k for k, v in ID_to_AU.items()}\n",
    "F0_to_ID = {v:k for k, v in ID_to_F0.items()}\n",
    "\n",
    "\n",
    "print(\"Vocabulary sizes\")\n",
    "print(f\"AU: {len(all_AU)}\")\n",
    "print(f\"F0: {len(all_f0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "formed-humanitarian",
   "metadata": {},
   "source": [
    "Make sequences of 100 frames\n",
    "\n",
    "Sequences are made with string values to avoid float issues comparing numbers and mixing numerical values with tags such as \"PAD\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "educated-document",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEQ_LEN = 100\n",
    "SEQ_OVERLAP = 2 # At least 2 sample overlap as we introduce SOS and EOS tokens for each sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "appreciated-stake",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 590/590 [00:03<00:00, 161.73it/s]\n"
     ]
    }
   ],
   "source": [
    "sequences_features = []\n",
    "sequences_predict = []\n",
    "\n",
    "# Create sequences independantly for every scene\n",
    "# Each sequence is made of indexes (translate after with ID_to_X)\n",
    "for i, (idscene_i, idscene_f) in enumerate(tqdm(scene_idxs)):\n",
    "    l = idscene_f - idscene_i\n",
    "    current_i = idscene_i\n",
    "    \n",
    "#     print(\"scene\", current_i, idscene_f)\n",
    "    # While possible to fit a whole sequence of SEQ_LEN, do it\n",
    "    while current_i + SEQ_LEN <= idscene_f:\n",
    "        # Sequence limits\n",
    "        id_i = current_i\n",
    "        id_f = current_i + SEQ_LEN\n",
    "        # Update current i\n",
    "        current_i = current_i + SEQ_LEN - SEQ_OVERLAP\n",
    "        \n",
    "        # Get data F0\n",
    "        seq_feature = np.array([F0_to_ID[str(i)] for i in df_AU[id_i:id_f][\"f0\"].values], dtype=int)\n",
    "        seq_predict = np.zeros((SEQ_LEN, len(AU_columns)), dtype=int)\n",
    "        # Get data AUs\n",
    "        predict_values = df_AU[id_i:id_f][AU_columns].values\n",
    "        for i in range(predict_values.shape[0]):\n",
    "            for j in range(predict_values.shape[1]):\n",
    "                seq_predict[i, j] = AU_to_ID[str(predict_values[i, j])]\n",
    "                \n",
    "        # Add SOS-EOS indexes\n",
    "        # SOS\n",
    "        seq_feature[0] = 1\n",
    "        seq_predict[0,:] = 1\n",
    "        # EOS\n",
    "        seq_feature[SEQ_LEN-1] = 2\n",
    "        seq_predict[SEQ_LEN-1,:] = 2\n",
    "        \n",
    "        # Save sequence\n",
    "        sequences_features.append(seq_feature)\n",
    "        sequences_predict.append(seq_predict)\n",
    "#         print(\"--> added\", id_i, id_f)\n",
    "        \n",
    "    # Handle last window with padding\n",
    "    if current_i != idscene_f - SEQ_OVERLAP:\n",
    "        last_frame_features = np.zeros((SEQ_LEN), dtype=int)\n",
    "        last_frame_predict = np.zeros((SEQ_LEN, len(AU_columns)), dtype=int)\n",
    "        # Get data F0\n",
    "        last_frame_features[:idscene_i+l-current_i] = np.array(\n",
    "            [F0_to_ID[str(i)] for i in df_AU[current_i:idscene_f][\"f0\"].values], \n",
    "                                                               dtype=int)\n",
    "        # Get data AUs\n",
    "        predict_values = df_AU[current_i:idscene_f][AU_columns].values\n",
    "        for i in range(predict_values.shape[0]):\n",
    "            for j in range(predict_values.shape[1]):\n",
    "                last_frame_predict[i, j] = AU_to_ID[str(predict_values[i, j])]        \n",
    "        \n",
    "        # Add SOS-EOS indexes\n",
    "        # SOS\n",
    "        last_frame_features[0] = 1\n",
    "        last_frame_predict[0,:] = 1\n",
    "        # EOS\n",
    "        last_frame_features[predict_values.shape[0]] = 2\n",
    "        last_frame_predict[predict_values.shape[0],:] = 2\n",
    "        \n",
    "        # Save sequence\n",
    "        sequences_features.append(last_frame_features)\n",
    "        sequences_predict.append(last_frame_predict)\n",
    "\n",
    "sequences_indexes = list(range(len(sequences_features)))\n",
    "sequences_features = np.array(sequences_features)\n",
    "sequences_predict = np.array(sequences_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "manufactured-utilization",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using sequences of length 100 with 2 overlapping samples\n",
      "Input features shape (3430, 100)\n",
      "Target features shape (3430, 100, 6)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Using sequences of length {SEQ_LEN} with {SEQ_OVERLAP} overlapping samples\")\n",
    "print(\"Input features shape\", sequences_features.shape)\n",
    "print(\"Target features shape\", sequences_predict.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "czech-bulgaria",
   "metadata": {},
   "source": [
    "### Make Train/Val/Test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "prescription-restaurant",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seq_to_onehot(sequence):\n",
    "    \"\"\"Trasnforms a sequence of indexes to a one-hot encoding of the same sequence. \n",
    "    Output array has 1 extra dimension of the size of the vocabulary\n",
    "    \"\"\"\n",
    "    # Infer output size from F0 or AU\n",
    "    if len(sequence.shape) == 1:\n",
    "        new_sequence = np.zeros((sequence.shape[0], len(F0_to_ID)))\n",
    "    else:\n",
    "        new_sequence = np.zeros((sequence.shape[0], sequence.shape[1], len(AU_to_ID)))\n",
    "    # Fill one-hot vectors\n",
    "    for i in range(sequence.shape[0]):\n",
    "        if len(sequence.shape) == 1:\n",
    "            sample_i = sequence[i]\n",
    "            new_sequence[i, sample_i] = 1.0\n",
    "        else:\n",
    "            for j in range(sequence.shape[1]):\n",
    "                sample_ij = sequence[i,j]\n",
    "                new_sequence[i, j, sample_ij] = 1.0\n",
    "    \n",
    "    return new_sequence\n",
    "\n",
    "def onehot_to_seq(onehot):\n",
    "    \"\"\"Transforms a one-hot encoding of an index array to an index array.\n",
    "    Output array has 1 less dimension\n",
    "    \"\"\"\n",
    "    # Infer output size from F0 or AU, return argmax to have index of one-hot\n",
    "    if len(onehot.shape) == 2:\n",
    "        return np.argmax(onehot, axis=1)\n",
    "    else:\n",
    "        return np.array([np.argmax(onehot_i, axis=1) for onehot_i in onehot])\n",
    "    \n",
    "def seq_to_real(sequence):\n",
    "    \"\"\" Returns the corresponding quantized values from a sequence of vocabulary indexes.\n",
    "    As there are no negative values in data, the following convention is used:\n",
    "     0: PAD\n",
    "    -1: SOS\n",
    "    -2: EOS\n",
    "    \"\"\"\n",
    "    # Create containing array\n",
    "    values = np.zeros_like(sequence, dtype=float)\n",
    "    # Fill array (F0 case)\n",
    "    if len(sequence.shape) == 1:\n",
    "        for i, s_i in enumerate(sequence):\n",
    "            val_i = ID_to_F0[s_i]\n",
    "            if val_i not in [\"PAD\", \"SOS\", \"EOS\"]:\n",
    "                values[i] = float(val_i)\n",
    "            else:\n",
    "                values[i] = -s_i\n",
    "    # Fill array (AUs case)\n",
    "    else:\n",
    "        for i in range(sequence.shape[0]):\n",
    "            for j in range(sequence.shape[1]):\n",
    "                val_ij = ID_to_AU[sequence[i, j]]\n",
    "                if val_ij not in [\"PAD\", \"SOS\", \"EOS\"]:\n",
    "                    values[i, j] = float(val_ij)\n",
    "                else:\n",
    "                    values[i, j] = -sequence[i, j]\n",
    "    return values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "located-superintendent",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 3430/3430 [00:04<00:00, 688.34it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████| 3430/3430 [00:01<00:00, 2109.47it/s]\n"
     ]
    }
   ],
   "source": [
    "# Sanity check\n",
    "for i, s in enumerate(tqdm(sequences_features)):\n",
    "    assert (s - onehot_to_seq(seq_to_onehot(s)) == np.zeros_like(s)).all(), i\n",
    "\n",
    "for i, s in enumerate(tqdm(sequences_predict)):\n",
    "    assert (s - onehot_to_seq(seq_to_onehot(s)) == np.zeros_like(s)).all(), i"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "similar-cotton",
   "metadata": {},
   "source": [
    "### seq2seq model\n",
    "\n",
    "For this Lab we'll work only with F0 as feature and we'll try to predict the Action units.\n",
    "\n",
    "Each sequence is starts with `SOS` and ends with `EOS`, eventually with `PAD`. The code of this section is inspired from [this tutorial](https://www.tensorflow.org/tutorials/text/nmt_with_attention)\n",
    "\n",
    "Dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "extended-executive",
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = len(sequences_features)\n",
    "BATCH_SIZE = 15\n",
    "embedding_dim = 256\n",
    "units = 1024\n",
    "vocab_inp_size = len(F0_to_ID)\n",
    "vocab_tar_size = len(AU_to_ID)\n",
    "\n",
    "AU_model = 0\n",
    "\n",
    "train_size = int(0.8 * BUFFER_SIZE)\n",
    "val_size = int(0.1 * BUFFER_SIZE)\n",
    "test_size = int(0.1 * BUFFER_SIZE)\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((sequences_features, sequences_predict[:, :, AU_model])).shuffle(BUFFER_SIZE)\n",
    "train_dataset = dataset.take(train_size)\n",
    "test_dataset = dataset.skip(train_size)\n",
    "val_dataset = test_dataset.skip(test_size)\n",
    "test_dataset = test_dataset.take(test_size)\n",
    "\n",
    "\n",
    "\n",
    "train_dataset = train_dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
    "val_dataset = val_dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
    "test_dataset = test_dataset.batch(BATCH_SIZE, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "internal-straight",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([15, 100]), TensorShape([15, 100]))"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_input_batch, example_target_batch = next(iter(train_dataset))\n",
    "example_input_batch.shape, example_target_batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "appreciated-aberdeen",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([15, 100]), TensorShape([15, 100]))"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_input_batch, example_target_batch = next(iter(val_dataset))\n",
    "example_input_batch.shape, example_target_batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "pleased-member",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([15, 100]), TensorShape([15, 100]))"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_input_batch, example_target_batch = next(iter(test_dataset))\n",
    "example_input_batch.shape, example_target_batch.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "affecting-grounds",
   "metadata": {},
   "source": [
    "Define model modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "early-disorder",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.batch_sz = batch_sz\n",
    "        self.enc_units = enc_units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = tf.keras.layers.GRU(self.enc_units,\n",
    "                                       return_sequences=True,\n",
    "                                       return_state=True,\n",
    "                                       recurrent_initializer='glorot_uniform')\n",
    "\n",
    "    def call(self, x, hidden):\n",
    "        x = self.embedding(x)\n",
    "        output, state = self.gru(x, initial_state = hidden)\n",
    "        return output, state\n",
    "\n",
    "    def initialize_hidden_state(self):\n",
    "        return tf.zeros((self.batch_sz, self.enc_units))\n",
    "    \n",
    "class BahdanauAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, units):\n",
    "        super(BahdanauAttention, self).__init__()\n",
    "        self.W1 = tf.keras.layers.Dense(units)\n",
    "        self.W2 = tf.keras.layers.Dense(units)\n",
    "        self.V = tf.keras.layers.Dense(1)\n",
    "\n",
    "    def call(self, query, values):\n",
    "        # query hidden state shape == (batch_size, hidden size)\n",
    "        # query_with_time_axis shape == (batch_size, 1, hidden size)\n",
    "        # values shape == (batch_size, max_len, hidden size)\n",
    "        # we are doing this to broadcast addition along the time axis to calculate the score\n",
    "        query_with_time_axis = tf.expand_dims(query, 1)\n",
    "\n",
    "        # score shape == (batch_size, max_length, 1)\n",
    "        # we get 1 at the last axis because we are applying score to self.V\n",
    "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
    "        score = self.V(tf.nn.tanh(\n",
    "            self.W1(query_with_time_axis) + self.W2(values)))\n",
    "\n",
    "        # attention_weights shape == (batch_size, max_length, 1)\n",
    "        attention_weights = tf.nn.softmax(score, axis=1)\n",
    "\n",
    "        # context_vector shape after sum == (batch_size, hidden_size)\n",
    "        context_vector = attention_weights * values\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "\n",
    "        return context_vector, attention_weights\n",
    "\n",
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.batch_sz = batch_sz\n",
    "        self.dec_units = dec_units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = tf.keras.layers.GRU(self.dec_units,\n",
    "                                       return_sequences=True,\n",
    "                                       return_state=True,\n",
    "                                       recurrent_initializer='glorot_uniform')\n",
    "        self.fc = tf.keras.layers.Dense(vocab_size)\n",
    "\n",
    "    # used for attention\n",
    "        self.attention = BahdanauAttention(self.dec_units)\n",
    "\n",
    "    def call(self, x, hidden, enc_output):\n",
    "        # enc_output shape == (batch_size, max_length, hidden_size)\n",
    "        context_vector, attention_weights = self.attention(hidden, enc_output)\n",
    "\n",
    "        # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n",
    "        x = self.embedding(x)\n",
    "\n",
    "        # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n",
    "        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
    "\n",
    "        # passing the concatenated vector to the GRU\n",
    "        output, state = self.gru(x)\n",
    "\n",
    "        # output shape == (batch_size * 1, hidden_size)\n",
    "        output = tf.reshape(output, (-1, output.shape[2]))\n",
    "\n",
    "        # output shape == (batch_size, vocab)\n",
    "        x = self.fc(output)\n",
    "\n",
    "        return x, state, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "exposed-thing",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder output shape: (batch size, sequence length, units) (15, 100, 1024)\n",
      "Encoder Hidden state shape: (batch size, units) (15, 1024)\n"
     ]
    }
   ],
   "source": [
    "encoder = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE)\n",
    "\n",
    "# sample input\n",
    "sample_hidden = encoder.initialize_hidden_state()\n",
    "sample_output, sample_hidden = encoder(example_input_batch, sample_hidden)\n",
    "print ('Encoder output shape: (batch size, sequence length, units) {}'.format(sample_output.shape))\n",
    "print ('Encoder Hidden state shape: (batch size, units) {}'.format(sample_hidden.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "valuable-princess",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention result shape: (batch size, units) (15, 1024)\n",
      "Attention weights shape: (batch_size, sequence_length, 1) (15, 100, 1)\n"
     ]
    }
   ],
   "source": [
    "attention_layer = BahdanauAttention(10)\n",
    "attention_result, attention_weights = attention_layer(sample_hidden, sample_output)\n",
    "\n",
    "print(\"Attention result shape: (batch size, units) {}\".format(attention_result.shape))\n",
    "print(\"Attention weights shape: (batch_size, sequence_length, 1) {}\".format(attention_weights.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "tropical-barrier",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoder output shape: (batch_size, vocab size) (15, 29)\n"
     ]
    }
   ],
   "source": [
    "decoder = Decoder(vocab_tar_size, embedding_dim, units, BATCH_SIZE)\n",
    "\n",
    "sample_decoder_output, _, _ = decoder(tf.random.uniform((BATCH_SIZE, 1)),\n",
    "                                      sample_hidden, sample_output)\n",
    "\n",
    "print ('Decoder output shape: (batch_size, vocab size) {}'.format(sample_decoder_output.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "assumed-browse",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "\n",
    "    return tf.reduce_mean(loss_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "beginning-brooks",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir = './training_checkpoints'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
    "                                 encoder=encoder,\n",
    "                                 decoder=decoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "listed-purchase",
   "metadata": {},
   "source": [
    "Train function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "lightweight-listing",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(inp, targ, enc_hidden):\n",
    "    loss = 0\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        enc_output, enc_hidden = encoder(inp, enc_hidden)\n",
    "\n",
    "        dec_hidden = enc_hidden\n",
    "\n",
    "        dec_input = tf.expand_dims([AU_to_ID['SOS']] * BATCH_SIZE, 1)\n",
    "\n",
    "        # Teacher forcing - feeding the target as the next input\n",
    "        for t in range(1, targ.shape[1]):\n",
    "            # passing enc_output to the decoder\n",
    "            predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n",
    "\n",
    "            loss += loss_function(targ[:, t], predictions)\n",
    "\n",
    "            # using teacher forcing\n",
    "            dec_input = tf.expand_dims(targ[:, t], 1)\n",
    "\n",
    "    batch_loss = (loss / int(targ.shape[1]))\n",
    "\n",
    "    variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "\n",
    "    gradients = tape.gradient(loss, variables)\n",
    "\n",
    "    optimizer.apply_gradients(zip(gradients, variables))\n",
    "\n",
    "    return batch_loss\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "arctic-marker",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def validation_step(inp, targ, enc_hidden):\n",
    "    loss = 0\n",
    "\n",
    "    enc_output, enc_hidden = encoder(inp, enc_hidden)\n",
    "\n",
    "    dec_hidden = enc_hidden\n",
    "\n",
    "    dec_input = tf.expand_dims([AU_to_ID['SOS']] * BATCH_SIZE, 1)\n",
    "\n",
    "    # Teacher forcing - feeding the target as the next input\n",
    "    for t in range(1, targ.shape[1]):\n",
    "        # passing enc_output to the decoder\n",
    "        predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n",
    "\n",
    "        loss += loss_function(targ[:, t], predictions)\n",
    "\n",
    "        # using teacher forcing\n",
    "        dec_input = tf.expand_dims(targ[:, t], 1)\n",
    "\n",
    "    batch_loss = (loss / int(targ.shape[1]))\n",
    "\n",
    "    return batch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "toxic-occurrence",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                            | 0/5 [00:00<?, ?it/s]\n",
      "0it [00:00, ?it/s]\u001b[A\n",
      "1it [00:09,  9.38s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Batch 0 Loss 2.9627\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "2it [00:18,  9.20s/it]\u001b[A\n",
      "3it [00:27,  9.16s/it]\u001b[A\n",
      "4it [00:36,  9.20s/it]\u001b[A\n",
      "5it [00:46,  9.25s/it]\u001b[A\n",
      "6it [00:55,  9.31s/it]\u001b[A\n",
      "7it [01:05,  9.40s/it]\u001b[A\n",
      "8it [01:15,  9.58s/it]\u001b[A\n",
      "9it [01:24,  9.54s/it]\u001b[A\n",
      "10it [01:34,  9.53s/it]\u001b[A\n",
      "11it [01:43,  9.48s/it]\u001b[A\n",
      "12it [01:52,  9.42s/it]\u001b[A\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A\n",
      "1it [00:02,  2.91s/it]\u001b[A\n",
      " 20%|████████████████▌                                                                  | 1/5 [01:55<07:43, 115.91s/it]\n",
      "0it [00:00, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss 3.7407\n",
      "Time taken for 1 epoch 115.91215395927429 sec\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "1it [00:09,  9.58s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Batch 0 Loss 1.1815\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "2it [00:19,  9.79s/it]\u001b[A\n",
      "3it [00:29,  9.93s/it]\u001b[A\n",
      "4it [00:39,  9.79s/it]\u001b[A\n",
      "5it [00:48,  9.62s/it]\u001b[A\n",
      "6it [00:57,  9.55s/it]\u001b[A\n",
      "7it [01:07,  9.57s/it]\u001b[A\n",
      "8it [01:16,  9.47s/it]\u001b[A\n",
      "9it [01:26,  9.50s/it]\u001b[A\n",
      "10it [01:35,  9.43s/it]\u001b[A\n",
      "11it [01:45,  9.44s/it]\u001b[A\n",
      "12it [01:54,  9.55s/it]\u001b[A\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A\n",
      "1it [00:02,  2.93s/it]\u001b[A\n",
      " 40%|█████████████████████████████████▏                                                 | 2/5 [03:53<05:51, 117.09s/it]\n",
      "0it [00:00, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss 1.3693\n",
      "Time taken for 1 epoch 117.92104768753052 sec\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "1it [00:09,  9.31s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Batch 0 Loss 1.2824\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "2it [00:18,  9.36s/it]\u001b[A\n",
      "3it [00:28,  9.40s/it]\u001b[A\n",
      "4it [00:37,  9.43s/it]\u001b[A\n",
      "5it [00:47,  9.55s/it]\u001b[A\n",
      "6it [00:56,  9.46s/it]\u001b[A\n",
      "7it [01:05,  9.40s/it]\u001b[A\n",
      "8it [01:15,  9.59s/it]\u001b[A\n",
      "9it [01:25,  9.53s/it]\u001b[A\n",
      "10it [01:34,  9.53s/it]\u001b[A\n",
      "11it [01:44,  9.51s/it]\u001b[A\n",
      "12it [01:53,  9.48s/it]\u001b[A\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A\n",
      "1it [00:02,  2.93s/it]\u001b[A\n",
      " 60%|█████████████████████████████████████████████████▊                                 | 3/5 [05:50<03:53, 116.93s/it]\n",
      "0it [00:00, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss 1.0739\n",
      "Time taken for 1 epoch 116.74099969863892 sec\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "1it [00:09,  9.29s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Batch 0 Loss 1.1308\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "2it [00:18,  9.26s/it]\u001b[A\n",
      "3it [00:28,  9.36s/it]\u001b[A\n",
      "4it [00:37,  9.53s/it]\u001b[A\n",
      "5it [00:47,  9.69s/it]\u001b[A\n",
      "6it [00:57,  9.67s/it]\u001b[A\n",
      "7it [01:07,  9.78s/it]\u001b[A\n",
      "8it [01:17,  9.79s/it]\u001b[A\n",
      "9it [01:26,  9.76s/it]\u001b[A\n",
      "10it [01:36,  9.80s/it]\u001b[A\n",
      "11it [01:46,  9.72s/it]\u001b[A\n",
      "12it [01:55,  9.66s/it]\u001b[A\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A\n",
      "1it [00:02,  2.98s/it]\u001b[A\n",
      " 80%|██████████████████████████████████████████████████████████████████▍                | 4/5 [07:49<01:57, 117.88s/it]\n",
      "0it [00:00, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss 1.0293\n",
      "Time taken for 1 epoch 119.33600044250488 sec\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "1it [00:09,  9.37s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Batch 0 Loss 0.9279\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "2it [00:18,  9.34s/it]\u001b[A\n",
      "3it [00:28,  9.33s/it]\u001b[A\n",
      "4it [00:37,  9.45s/it]\u001b[A\n",
      "5it [00:47,  9.43s/it]\u001b[A\n",
      "6it [00:56,  9.51s/it]\u001b[A\n",
      "7it [01:06,  9.68s/it]\u001b[A\n",
      "8it [01:16,  9.84s/it]\u001b[A\n",
      "9it [01:26,  9.78s/it]\u001b[A\n",
      "10it [01:36,  9.77s/it]\u001b[A\n",
      "11it [01:45,  9.66s/it]\u001b[A\n",
      "12it [01:55,  9.60s/it]\u001b[A\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A\n",
      "1it [00:02,  2.96s/it]\u001b[A\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████| 5/5 [09:48<00:00, 117.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Loss 0.8654\n",
      "Time taken for 1 epoch 118.21458959579468 sec\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "losses_train = []\n",
    "losses_val = []\n",
    "EPOCHS = 5\n",
    "\n",
    "steps_per_epoch_train = train_dataset.cardinality().numpy()//BATCH_SIZE\n",
    "steps_per_epoch_val = val_dataset.cardinality().numpy()//BATCH_SIZE\n",
    "\n",
    "for epoch in tqdm(range(EPOCHS)):\n",
    "    start = time.time()\n",
    "\n",
    "    enc_hidden = encoder.initialize_hidden_state()\n",
    "    total_loss = 0\n",
    "\n",
    "    for (batch, (inp, targ)) in tqdm(enumerate(train_dataset.take(steps_per_epoch_train))):\n",
    "        batch_loss = train_step(inp, targ, enc_hidden)\n",
    "        total_loss += batch_loss\n",
    "        losses_train.append(batch_loss)\n",
    "\n",
    "        if batch % 20 == 0:\n",
    "            print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1,\n",
    "                                                       batch,\n",
    "                                                       batch_loss.numpy()))\n",
    "    # Validation:\n",
    "    for (batch, (inp, targ)) in tqdm(enumerate(val_dataset.take(steps_per_epoch_val))):\n",
    "        batch_val_loss = validation_step(inp, targ, enc_hidden)\n",
    "        losses_val.append(batch_val_loss)\n",
    "        \n",
    "  # saving (checkpoint) the model every 2 epochs\n",
    "    if (epoch + 1) % 2 == 0:\n",
    "        checkpoint.save(file_prefix = checkpoint_prefix)\n",
    "\n",
    "    print('Epoch {} Loss {:.4f}'.format(epoch + 1,\n",
    "                                      total_loss / steps_per_epoch_train))\n",
    "    print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "signed-filing",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "t_train = np.linspace(0, EPOCHS, len(losses_train))\n",
    "t_val = np.linspace(0, EPOCHS, len(losses_val))\n",
    "\n",
    "plt.plot(t_train, losses_train, label='Train Loss')\n",
    "plt.plot(t_val, losses_val, label='Validation Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "spatial-twins",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(sequence):\n",
    "    max_length_targ = 100\n",
    "    max_length_inp = 100\n",
    "    attention_plot = np.zeros((max_length_targ, max_length_inp))\n",
    "\n",
    "    inputs = sequence\n",
    "    result = np.zeros(max_length_targ, dtype=int)\n",
    "\n",
    "    hidden = [tf.zeros((1, units))]\n",
    "    enc_out, enc_hidden = encoder(inputs, hidden)\n",
    "\n",
    "    dec_hidden = enc_hidden\n",
    "    dec_input = tf.expand_dims([AU_to_ID['SOS']], 0)\n",
    "\n",
    "    for t in range(max_length_targ):\n",
    "        predictions, dec_hidden, attention_weights = decoder(dec_input,\n",
    "                                                             dec_hidden,\n",
    "                                                             enc_out)\n",
    "\n",
    "        # storing the attention weights to plot later on\n",
    "        attention_weights = tf.reshape(attention_weights, (-1, ))\n",
    "        attention_plot[t] = attention_weights.numpy()\n",
    "\n",
    "        predicted_id = tf.argmax(predictions[0]).numpy()\n",
    "\n",
    "        result[t] = predicted_id\n",
    "\n",
    "        if ID_to_AU[predicted_id] == 'EOS':\n",
    "            return result, sentence, attention_plot\n",
    "\n",
    "    # the predicted ID is fed back into the model\n",
    "        dec_input = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "    return result, attention_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "soviet-corps",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def evaluation_step(inp, targ, enc_hidden):\n",
    "    loss = 0\n",
    "    \n",
    "    enc_output, enc_hidden = encoder(inp, enc_hidden)\n",
    "\n",
    "    dec_hidden = enc_hidden\n",
    "\n",
    "    dec_input = tf.expand_dims([AU_to_ID['SOS']] * BATCH_SIZE, 1)\n",
    "\n",
    "    # Teacher forcing - feeding the target as the next input\n",
    "    preds = np.zeros_like(inp)\n",
    "    for t in range(1, targ.shape[1]):\n",
    "        # passing enc_output to the decoder\n",
    "        predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n",
    "\n",
    "        loss += loss_function(targ[:, t], predictions)\n",
    "        predicted_id = tf.argmax(predictions, axis=1).numpy()\n",
    "        preds[:,t] = predicted_id\n",
    "\n",
    "    batch_loss = (loss / int(targ.shape[1]))\n",
    "\n",
    "    return batch_loss, preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "separated-discovery",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:02,  2.87s/it]\n"
     ]
    }
   ],
   "source": [
    "steps_per_epoch_test = test_dataset.cardinality().numpy()//BATCH_SIZE\n",
    "\n",
    "\n",
    "enc_hidden = encoder.initialize_hidden_state()\n",
    "total_loss = 0\n",
    "\n",
    "predictions = []\n",
    "for (batch, (inp, targ)) in tqdm(enumerate(test_dataset.take(steps_per_epoch_test))):\n",
    "    batch_loss_test, preds = evaluation_step(inp, targ, enc_hidden)\n",
    "    total_loss += batch_loss_test\n",
    "    predictions.append(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "covered-trial",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss 1.698737382888794\n"
     ]
    }
   ],
   "source": [
    "print(f'Test loss {total_loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "ready-rider",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1660052c340>]"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWoAAAD4CAYAAADFAawfAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/Il7ecAAAACXBIWXMAAAsTAAALEwEAmpwYAAAYtUlEQVR4nO3dfZBkV1nH8d/Tb7PTs9lMN9loTLJskBh8oUJ0oMComKgQIgW+VFmhCrWQqi2rtAxIGZPiL63CvyzUP8ByVcQSBCkFpQIqEaWIGoFNSGAhiVGJIZCwG2cmm8zszu2+9/GPvre352V3+nb3vX2m5/up2ppMz3Tfc7O9vznz3HOea+4uAEC4KtMeAADg4ghqAAgcQQ0AgSOoASBwBDUABK5WxItedtllfvTo0SJeGgBm0v333/+Mux/e6WuFBPXRo0d14sSJIl4aAGaSmf3vhb5G6QMAAkdQA0DgCGoACBxBDQCBI6gBIHBDrfows8clPScpltR196UiBwUAOC/P8ryb3P2ZwkYCANhRIeuogd383YPf0H+fer604x2ar+stN16jasVyP/eTX35Kjzx1JtdzbjjS0k0vuTz3sYCdDBvULulTZuaS/sjdj2/9BjM7JumYJB05cmRyI8TM6caJ3v5XDypxyfLnZm5Zy/WXH23r+qsXcz7X9Y6PPKSznXjosbpLVy7O69/uvDnfQIELGDaob3T3b5rZ5ZLuMbNH3P2zg9+QhvdxSVpaWuJuBLig1bMdJS799hu/V7/wqqOFH++BJ1b0M+/9dy2vRbmfe7YT62wn1p2ve4l++dXfOdRz3vWJr+oD//FE7mMBFzLUqg93/2b68ZSkj0l6RZGDwmxbSQNzsdko5Xjt9DijBHX2nFazPvRzFpuNXsBHce7jATvZNajNbMHMLsn+W9JrJJ0semCYXVn4tUsK6lZ6nJX1/EG9stbZ9BrDaC+MfjxgJ8OUPr5N0sesV6CrSfpLd/+HQkeFmbaynobfwvCz1HFccqCmasVGC+r0OVn4DmPwB8N3LM7nPiaw1a5B7e7/I+n6EsaCfWKU8BtHpWJqNetaTmfHeWRjbeUYa39GPcLxgJ2wMxGlO1/3LSeos2OtjFGjzlOmaae/KSxT+sCEENQo3cpapGajqgP1amnHbC00RgrOlbVIFeutwx76WFnpY4QfDMBOCGqUbnk9KnU2LfVmxCPNqNcjLTYbuTbKXDpfl9loq0yAnRDUKN3KWlRafTrTWmiMvOojz9I8SapVK7p0vs6qD0wMQY3SLa93cl2cm4T2Ql0r6x0lSb69WMsj/lBpNxvMqDExBDVKt7IWqZ1zljquVrOhOHE9d66b63krI5ZpRp3BAzshqFG6lbWotF2JmWxWnPeC4vLaiEE94nJAYCcENUoVdRM9t9GdSo1ayneBz917M+oRxjrqckBgJwQ1SrV6Nv8GkknI1kGv5phRr0WxOrH310XnOl5a+nCnPxnGR1CjVNluvbL6fGTaI8yoV8bYmNNaaGijm+hsh8ZMGB9BjVL1dyWW1Ocjs5hevMxzga+/K3HEVR+DrwGMg6BGqcru85E5OFdTvWq5LvAtj9DnI9Oi3wcmiKBGqcpucZoxs9wX+FbGGCv9PjBJBDVKVfZNAwa1c/b7OF+mGW3Vh0S/D0wGQY1SLa9HumSupkat/Lde7hn1eqRqxXToQP57QI9y8RK4EIIapVpZG21d8iTkn1F31Go2ZCPcgffQgboqxl1eMBkENUq1vJ6/ydGktBbquWvUo461UjEt0u8DE0JQo1RTnVE3G1o921E8ZGOm5RF3JWZaTTroYTIIapRqZT0qfcVHprXQkLt05uxwS+ZWxxxre6HB8jxMBEGNUk27Ri0Nv2RueW28dqytJh30MBkENUpzrhNrLYpL3+ySybNkLmvINEqfj0x7gRo1JoOgRmlW13tlgLJvw5XJs2TuzLmu4sTHGmuLxkyYEIIapTnfO2Naqz7SGfUQ5YiVMfp8ZNrNhjqx6/mNfDcrALYiqFGaLCCnNqPuN0ra/QLfOH0+MvT7wKQQ1CjNON3oJmG+UdVcrZJvRj3Wqg/6fWAyCGqUZmUCs9RxDXuBbxI/VOj3gUkhqFGaLPwW56dTo5aG7/eR/VBZHGMXZYue1JgQghqlWVmLdOhATbXq9N52w/b7WF7rqF41HZzL35Apk+fiJXAxBDVKs7LemVp9OtNaaPSXCV7M6no0ckOmzKEDNVUrRlBjbAQ1SjPqHb0nqd2sD12jHveHSnazgjx3lQF2QlCjNMtr0+vzkWktNPTs2Y66cXLR71tJZ9Tjaufs2AfsZOigNrOqmX3RzO4uckCYXdPs85HJZsmruzRmmsSMWupdUGR5HsaVZ0Z9u6SHixoIZt/y+mTCbxzDLplbWe9M5E7pvQ56BDXGM9QlbTO7StJPSnqXpF8vdESpt77/C7r3sWcm/rqthbo+9fZX69IpLhHbL86c6+g17/5svyYcxcnUg/oF6fFf9wf3qnKRC4W9sc6Nfbz2QkOPnXpe3/XOv5ckvejwgv7+9h8e6yIl9p9h1x79vqQ7JF1yoW8ws2OSjknSkSNHxh7YQ08+qxdfflCvvu7w2K+VeeL/1vWJLz+lry+v69IrL53Y62JnT/zfup4+c063vvTb9cIXLKhWMf30DVdOdUw/cLSl33jtdbv236ia6eeWrhr7eG+58RpdOl+XSzr5jWd172PP6My5LhMF5LJrUJvZ6yWdcvf7zexHL/R97n5c0nFJWlpaGrtdWCdO9Ipr2vrNW14y7kv1nXh8WZ/48lNsQChJtiztLTdeo5cfbU95ND1ztap+5aYXl3a8F19+UHek7+GPPvCk7n3sGa2sRQQ1chmmRn2jpDeY2eOSPizpZjP7QKGjkhR1E9Wrk/31kA0I5cp+IE6rCVNoWjlvXABkdg1qd7/L3a9y96OSbpP0z+7+5qIHFsWJGrXJrh5ss6W3VJNoFTpL2vT+wIiCXEcdJ644cTWq1Ym+7qH5uirGP5SyLK93ZCZ+zU/luXEBMChXIwN3/4ykzxQykgGddDNCvTbZ0ke1Yrp0vs6vniXJarHVCiscpPMNnii9Ia8gZ9Qb3V5QNwpo3tO7PRJbesswzTuOh+jgXE31qvH+Q25BBnU2o550jVrq1QkpfZQjhN4eIcl6f/D+Q15BBnVU8IyaGmE5ltc6rPjYgjuTYxRBBnXhM2pqhKVYWYumdiPbULV4/2EEQQZ1NqOuF1WjXuvIfew9ObgId9cypY9tmFFjFEEGdf9iYhEz6oW6ojjRWhRP/LVx3noUK+omXEzcorVQ52IicgsyqIssfXDD0XL0dyUyo96k3WxodT1SnPAbHYYXZFAXeTGRTQflyOqwzKg3ay00lLh0Zpd+2MCgMIO6yBk1/RZKwYx6Z23efxhBkEHdL30UcTGR0kcpshl1q8mqj0GLvP8wgiCDushVHzRmKkd2Q1caMm3G+w+jCDKoi1z1ccmBmqoV0ypX3gu1uh6pYtKhA8yoB2W39+L9hzyCDOpO3LsiXkTpo1IxtZo0Zira8lrvLt4VGjJtQo0aowgyqKMCZ9SS6LdQAvp87Gy+XtVcrcL7D7kEGdRFrqOW6PdRhuU1OuftxMzYnYjcggzq8xcTi/m1mX4fxVtZ6/TrsdiMfh/IK8ygLmVGzcWcIi2vR6z4uABm1MgrzKAucGei1Ov3sbIe0ZipIO6ulfRiIrbj5hXIK8ygjnt3IDcrpvTRajYUJ64z57qFvP5+99xGV93EmVFfQLtZZ0aNXMIM6m5S2GxaYndi0bL/r4vMqHe02Gzo2bMdddMSH7CbIIO6EyeqF1SflljLWrRstshNA3aWvf9WacyEIQUZ1IXPqLN/KAR1IbJdd9Sod8b7D3mFG9RFzqj7/RaY0RTh/IyaoN4J7z/kFWZQx0XPqHu/klOjLka/cx5BvaPs/ccFRQwrzKAueEZ9cK6metWoURdkeS1SrWK6ZK427aEEKftNg00vGFaQQd2Jiw1qM6PfR4GyPh9FLa/c61q0OkVOQQZ1bx11sUNjd1hx6PNxcQfqVTUbVSYKGFqYQV3wqg+JfgtFos/H7lrNBqU3DC3MoI690NKHxIy6SPT52F17gdIbhhdmUHeLL30sNuv0WyjIylrErsRdLDbrWub9hyHtmoZmdsDMPm9mD5nZV8zst4oeVNSNNVfCjHp1PVKS0JhpkpLEtbJOjXo3zKiRxzBpuCHpZne/XtLLJN1iZq8sclCd2AvrRZ1pNRtKXDpzjlnNJD13rqvEWUO9G66RII9dF7p6rxfo8+mn9fRPodPQqJvouo0vSffdV9gxXv70s/ql6jf0wEe+oGajWthx9puzUaxfqj6jlz/9kHTf4rSHE6ybV0+r0jmt+z74ebGKcW+7cnFeV7eavU/q89LSWyZ+jKF2JJhZVdL9kl4s6T3u/rkdvueYpGOSdOTIkbEGFcWJbnvyd6SvPT3W61zMSyW9tC7p8cIOsW/dVJd0Mv2DHf2IpB+pS3ps2iPBRC1cPr2gdvdY0svMbFHSx8zs+9z95JbvOS7puCQtLS2NNePudBM16uekG94sveZd47zURZ3tJP27yWByGtWK5utBXqcOyloUq8s1kj3t9+55VJ9++JTuveOm3gMF/XqUa4+vu6+a2Wck3aIC50sbcaJqrSPNHZLmF4s6jObnpfnCXh24uAXefHveodaynjy3qm7jkGoFrlQbZtXH4XQmLTObl/Tjkh4pakDurk6cqJZ0pCqbJgCEq92sy116tuDe4sPMqK+Q9Odpnboi6SPufndRA+omLndXzSOpOlfUYQBgbK2BBlsvOFhcXg2z6uNLkm4obARbRN1ENcW9T2os8QIQrv7dogruLR7cFZ9OnKih9KazVYIaQLjK6oQYXFBH3UQNpT+dKH0ACFhZvcWDC+qNbqJ6NqOm9AEgYPt2Rt2JE80ZpQ8A4ZtvVDVfrxZ+o+LggjqKKX0A2Dt6LZP32cXEaLD0wTpqAIHrtUzeZzPqTas+asyoAYStjJuQBBfUmy4mUqMGELgyWtYGF9Sd2DVnWY2aoAYQtn05o+6to6b0AWBvaDUbeu5cV50CO3EGGdSUPgDsFe2F3qKHIssfwQV1Z9PyPIIaQNiyxkyrBd6sOLigpvQBYC9pl7A7Mbig3ogT1dmZCGCP6Lc63U9B3ekmmqP0AWCP6Lc63U816mjThheCGkDYFpvpxcT9NKNm1QeAvWSuVtXBuVqh/T6CC+pe9zyaMgHYO1oLxfb7CC6oo26iA5VYsopUzXWTdACYinaz2N2J4QV1nAY1ZQ8Ae8Riwf0+wgvqbqJ561L2ALBntBf2YVDPWcyKDwB7RqvZ0Mp+u5h4oNKl9AFgz2gv1PX8Rlcb3biQ1w8uqKM40ZwIagB7R9H9PsIL6m6ihhHUAPaOovt9hBfUsffuQk6NGsAeUXS/j/CCuhv32pyy6gPAHlF0v48AgzpRw1hHDWDvaDX32Yy6E3tvRk3pA8AekTVmKqrfR3BB3W/KROkDwB5Rr1Z06ECtsE0vwQV1J05U945UrU97KAAwtCJ3J+4a1GZ2tZn9i5k9bGZfMbPbCxlJaqObqKYOt+ECsKe0FoprzDRMe7qupHe4+wNmdomk+83sHnf/ahEDimJKHwD2nnazoW89d66Q1951Ru3uT7n7A+l/PyfpYUlXFjIa9UofNUofAPaYxQL7feRq+GxmRyXdIOlzO3ztmKRjknTkyJGRBxR1E9XqEaUPAHvKD197mS47WMxqtaEvJprZQUl/I+lt7n5m69fd/bi7L7n70uHDh0ceUNRNVHW2kAPYW37qhit1163fXchrDxXUZlZXL6Q/6O4fLWQkkpLE1U1cVY8IagBIDbPqwyT9qaSH3f3dRQ4mihOZElU9pvQBAKlhZtQ3Svp5STeb2YPpn1uLGEwUJ2r070DOxUQAkIa4mOju/yrJShiLOt3BoGZGDQBSYDsT+2uoJUofAJAKKqg73bQhk0TpAwBSQQV1FMe9u7tIlD4AIBVUUG90B0sfLM8DACmwoO7Errl+6YOgBgApsKCOWPUBANsEF9R11lEDwCZBBXUnTtSwtPTB8jwAkBRYUG9Q+gCAbYIK6g5byAFgm6CCuncxkdIHAAwKK6gHt5CzPA8AJAUW1L2LiQQ1AAwKKqgpfQDAdkEF9aYt5MyoAUBSYEHdiRPNEdQAsElQQR11Ex2oENQAMCjAoI6lSk2qBDU0AJiaoNKwEyc6YF12JQLAgKCCOorT0ge9qAGgL6yg7rrmLKY+DQADwgpqSh8AsE1YQd2N1bCY0gcADAgqqDuxa846lD4AYEBQQR110w0vBDUA9AUX1HV16fMBAAPCCursVlzMqAGgL6ygzm7FRVADQF9YQZ3dOICgBoC+oIK6Eyeqe4fleQAwYNegNrP3mdkpMztZ9GCibhrUbHgBgL5hZtTvl3RLweOQ1JtRVyl9AMAmuwa1u39W0nIJY+nd4cUjSh8AMGBiNWozO2ZmJ8zsxOnTp0d6jaibqOr0+gCAQRMLanc/7u5L7r50+PDhkV7jn3791ZqzrlStT2pYALDn1aY9gEFXt+al7gY7EwFgQFDL85TEkpyLiQAwYJjleR+SdJ+k68zsSTN7a2GjiTd6HwlqAOjbtfTh7m8qYyCSemUPidIHAAwIq/QRd3ofuZgIAH2BBXVW+mBGDQCZwII6nVFT+gCAvrCCOqtRU/oAgL6wgprSBwBsE1hQZ6UPlucBQCasoO6yjhoAtgorqCl9AMA2gQU166gBYKuwgpqdiQCwTVhBHUe9j5Q+AKAv0KCm9AEAmbCCmtIHAGwTVlD3LyayPA8AMoEFNeuoAWCrsIK6m9aoKX0AQF9YQR1HkkyqBHUrRwCYqsCCeqNX9jCb9kgAIBhhBXU3ouwBAFuEFdRxxBpqANgisKDeYFciAGwRVlB3I3pRA8AWYQV1HLGGGgC2CDCoKX0AwKDwgprSBwBsElZQdzcofQDAFmEFNTVqANgmvKBmwwsAbBJWUHeZUQPAVmEFdUyNGgC2CiyomVEDwFZDBbWZ3WJmj5rZf5nZnYWNhp2JALDNrkFtZlVJ75H0OknfI+lNZvY9hYyGXh8AsM0wM+pXSPovd/8fd48kfVjSGwsZTdyh9AEAWwwT1FdK+vrA50+mj21iZsfM7ISZnTh9+vRoo7nuVumK60d7LgDMqGHuebXT7VZ82wPuxyUdl6SlpaVtXx/Kz/7xSE8DgFk2zIz6SUlXD3x+laRvFjMcAMBWwwT1FyRda2bXmFlD0m2SPl7ssAAAmV1LH+7eNbNflfSPkqqS3ufuXyl8ZAAAScPVqOXun5T0yYLHAgDYQVg7EwEA2xDUABA4ghoAAkdQA0DgzH20vSkXfVGz05L+d8SnXybpmQkOZy/Yj+cs7c/z3o/nLO3P8857zi9098M7faGQoB6HmZ1w96Vpj6NM+/Gcpf153vvxnKX9ed6TPGdKHwAQOIIaAAIXYlAfn/YApmA/nrO0P897P56ztD/Pe2LnHFyNGgCwWYgzagDAAIIaAAIXTFCXdgPdKTOzq83sX8zsYTP7ipndnj7eNrN7zOyx9GNr2mOdNDOrmtkXzezu9PP9cM6LZvbXZvZI+nf+qlk/bzN7e/rePmlmHzKzA7N4zmb2PjM7ZWYnBx674Hma2V1pvj1qZq/Nc6wggrrUG+hOX1fSO9z9uyW9UtKvpOd6p6RPu/u1kj6dfj5rbpf08MDn++Gc/0DSP7j7SyRdr975z+x5m9mVkn5N0pK7f596rZFv02ye8/sl3bLlsR3PM/03fpuk702f894094bj7lP/I+lVkv5x4PO7JN017XGVdO5/J+knJD0q6Yr0sSskPTrtsU34PK9K37g3S7o7fWzWz/mQpK8pvWg/8PjMnrfO32O1rV4b5bslvWZWz1nSUUknd/u73Zpp6vX3f9WwxwliRq0hb6A7a8zsqKQbJH1O0re5+1OSlH68fIpDK8LvS7pDUjLw2Kyf84sknZb0Z2nJ50/MbEEzfN7u/g1JvyvpCUlPSXrW3T+lGT7nLS50nmNlXChBPdQNdGeJmR2U9DeS3ubuZ6Y9niKZ2eslnXL3+6c9lpLVJH2/pD909xskrWk2fuW/oLQm+0ZJ10j6DkkLZvbm6Y4qCGNlXChBva9uoGtmdfVC+oPu/tH04W+Z2RXp16+QdGpa4yvAjZLeYGaPS/qwpJvN7AOa7XOWeu/rJ939c+nnf61ecM/yef+4pK+5+2l370j6qKQf1Gyf86ALnedYGRdKUO+bG+iamUn6U0kPu/u7B770cUm/mP73L6pXu54J7n6Xu1/l7kfV+7v9Z3d/s2b4nCXJ3Z+W9HUzuy596MckfVWzfd5PSHqlmTXT9/qPqXcBdZbPedCFzvPjkm4zszkzu0bStZI+P/SrTrsYP1Bcv1XSf0r6b0nvnPZ4CjzPH1LvV54vSXow/XOrpBeod7HtsfRje9pjLej8f1TnLybO/DlLepmkE+nf999Kas36eUv6LUmPSDop6S8kzc3iOUv6kHp1+I56M+a3Xuw8Jb0zzbdHJb0uz7HYQg4AgQul9AEAuACCGgACR1ADQOAIagAIHEENAIEjqAEgcAQ1AATu/wEvpngOZJKKYQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(targ[2])\n",
    "plt.plot(preds[2])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
